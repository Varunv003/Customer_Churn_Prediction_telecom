# -*- coding: utf-8 -*-
"""customer-churn-96-acc-smote-hp-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fW544XX897gCNiq_8TQpnXshIwcYp5FZ

# Import Statements
"""

# Commented out IPython magic to ensure Python compatibility.
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.ticker as mtick
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from imblearn.combine import SMOTEENN


# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# Load Dataset"""

data = pd.read_csv("/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv")

data.head()

data.shape

data.info()

data.describe()

100*data['Churn'].value_counts()/len(data['Churn'])

"""dataset is imbalanced"""

data['Churn'].value_counts()

"""# Cleaning"""

df = data.copy()

df = df.drop(['customerID'], axis=1)

df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors="coerce")
# convert object to numeric values, if error it will convert it to NaN value

df.isnull().sum()

"""handelling the missing value"""

df.dropna(subset=['TotalCharges'], inplace=True)
print(data.isnull().sum())

print(df['tenure'].max())

"""highest value of tenure is 72, its better to divide it in ranges,
so making a new column, range is 12 months, like 1-12 months, 13-24 months
"""

labels = ["{0} - {1}".format(i, i + 11) for i in range(1, 72, 12)]

df['tenure_group'] = pd.cut(df.tenure, range(1, 80, 12), right=False, labels=labels)

df['tenure_group'].value_counts()

"""Now we have tenure grp column, we can
drop the old tenure column,
"""

df = df.drop(['tenure'], axis=1)

df.head()

df["Churn"] = [1 if i=="Yes" else 0 for i in df["Churn"]]

df.head()

"""# Plots"""

for i, predictor in enumerate(df.drop(columns=['Churn', 'TotalCharges', 'MonthlyCharges'])):
    plt.figure(i)
    sns.countplot(data=df, x=predictor, hue='Churn')

"""# One Hot Encoding for Categorical Variables"""

df_dummies = pd.get_dummies(df)

df_dummies.head()

"""# Correlation"""

plt.figure(figsize=(20,8))
df_dummies.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')

"""# Some Insights

* People leaving more when they have month-to-month contracts.
* If there's no online security or tech support, customers are more likely to leave.
* People tend to stay longer when they have long-term contracts.
* If the subscription doesn't include internet, there's less chance of people leaving.
* New subscribers in their first year tend to stop using the service.
* Customers with fiber optics internet are more likely to leave.
"""

df_dummies.to_csv('tel_churn_encoded.csv')

new_df = df_dummies.copy()

"""# Train Test Split"""

new_df.head()

x=new_df.drop('Churn',axis=1)
x

y=df['Churn']
y

X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.25, random_state = 4)

"""# Trying Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
model = LogisticRegression(random_state=42,max_iter=1000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(classification_rep)

"""# Low accuracy because of Imbalanced dataset

* precision, recall and f1 score is okk for '0' that is 'not_churn'
* but not performing well on '1', because of imbalanced dataset
* as we have seen above
* Churn:-

*    No  ->   73.463013
*    Yes  ->  26.536987

# Using SMOTE to make the data balanced
"""

from imblearn.combine import SMOTEENN

sm = SMOTEENN()
X_resampled, y_resampled = sm.fit_resample(x, y)

xr_train,xr_test,yr_train,yr_test=train_test_split(X_resampled, y_resampled,test_size=0.25)

model_sm = LogisticRegression(random_state=42,max_iter=1000)
model_sm.fit(xr_train, yr_train)
y_pred_sm = model_sm.predict(xr_test)

accuracy_lr = accuracy_score(yr_test, y_pred_sm)
conf_matrix_lr = confusion_matrix(yr_test, y_pred_sm)
classification_rep_lr = classification_report(yr_test, y_pred_sm)

"""# Got Better Accuracy with SMOTE"""

print(f"Accuracy: {accuracy_lr:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix_lr)
print("\nClassification Report:")
print(classification_rep_lr)

"""# Trying multiple Models"""

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

models = {
    'SVM': SVC(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42)
}

results = {}

for model_name, model in models.items():
    model.fit(xr_train, yr_train)
    yr_pred = model.predict(xr_test)
    accuracy = accuracy_score(yr_test, yr_pred)
    classification_rep = classification_report(yr_test, yr_pred)
    conf_matrix = confusion_matrix(yr_test, yr_pred)

    results[model_name] = {
        'Accuracy': accuracy,
        'Classification Report': classification_rep,
        'Confusion Matrix': conf_matrix
    }

results['Logistic Regression'] = {
    'Accuracy': accuracy_lr,
    'Classification Report': classification_rep_lr,
    'Confusion Matrix': conf_matrix_lr
}

for model_name, result in results.items():
    print(f"Model: {model_name}")
    print(f"Accuracy: {result['Accuracy']:.4f}")
    print("Classification Report:")
    print(result['Classification Report'])
    print("Confusion Matrix:")
    print(result['Confusion Matrix'])
    print("\n" + "="*50 + "\n")

plt.figure(figsize=(12, 6))
sns.barplot(x=list(results.keys()), y=[result['Accuracy'] for result in results.values()])
plt.title('Model Comparison - Accuracy')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.show()

"""# Hyper parameter tuning"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}
gb_model = GradientBoostingClassifier()

grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=5, verbose=1)

grid_search.fit(xr_train, yr_train)

best_model = grid_search.best_estimator_

y_pred_test = best_model.predict(xr_test)
accuracy_test = accuracy_score(yr_test, y_pred_test)

print("Accuracy: ", accuracy_test)
print("Confusion Matrix:\n", confusion_matrix(yr_test, y_pred_test))
print("\nClassification Report:\n", classification_report(yr_test, y_pred_test))

from sklearn.ensemble import RandomForestClassifier

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf_model = RandomForestClassifier()
grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=5, verbose=1)
grid_search_rf.fit(xr_train, yr_train)

best_model_r = grid_search.best_estimator_

y_pred_test_r = best_model_r.predict(xr_test)
accuracy_test_r = accuracy_score(yr_test, y_pred_test_r)

print("Accuracy: ", accuracy_test)
print("Confusion Matrix:\n", confusion_matrix(yr_test, y_pred_test_r))
print("\nClassification Report:\n", classification_report(yr_test, y_pred_test_r))

"""# TO save the models"""

# to save model
# import joblib
# best_model_name = max(results, key=lambda k: results[k]['Accuracy'])
# best_model = models[best_model_name]
# joblib.dump(best_model, f'{best_model_name}_model.joblib')
# results.pop(best_model_name)
# second_best_model_name = max(results, key=lambda k: results[k]['Accuracy'])
# second_best_model = models[second_best_model_name]
# joblib.dump(second_best_model, f'{second_best_model_name}_model.joblib')

# loaded_best_model = joblib.load('/kaggle/working/Gradient Boosting_model.joblib')
# loaded_second_best_model = joblib.load('/kaggle/working/Random Forest_model.joblib')

# acc_boost = loaded_best_model.predict(xr_test)
# acc_random = loaded_second_best_model.predict(xr_test)

# print(f"Gradient_boost: {accuracy_score(yr_test, acc_boost)}")
# print(f"Gradient_boost: {accuracy_score(yr_test, acc_random)}")

"""# Trying Neural Network"""

# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense
# from sklearn.preprocessing import StandardScaler
# from sklearn.model_selection import StratifiedKFold
# from sklearn.metrics import accuracy_score
# from keras.callbacks import EarlyStopping

"""using kfold for model validation"""

# kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# accuracy_scores = []
# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)


# for train_index, test_index in kf.split(X_resampled, y_resampled):
#     X_train_fold, X_test_fold = X_resampled.iloc[train_index], X_resampled.iloc[test_index]
#     y_train_fold, y_test_fold = y_resampled.iloc[train_index], y_resampled.iloc[test_index]

#     scaler = StandardScaler()
#     X_train_fold_scaled = scaler.fit_transform(X_train_fold)
#     X_test_fold_scaled = scaler.transform(X_test_fold)

#     model = Sequential()
#     model.add(Dense(units=20, activation='relu', input_dim=X_resampled.shape[1]))
#     model.add(Dense(units=10, activation='relu'))
#     model.add(Dense(units=5, activation='relu'))
#     model.add(Dense(units=1, activation='sigmoid'))

#     model.compile(optimizer='adam',
#                   loss='binary_crossentropy',
#                   metrics=['accuracy'])

#     model.fit(
#         X_train_fold_scaled, y_train_fold,
#         epochs=25, batch_size=16,
#         validation_data=(X_test_fold_scaled, y_test_fold),
#         callbacks=[early_stopping],
#         verbose=1
#     )

#     y_pred_fold_prob = model.predict(X_test_fold_scaled)
#     y_pred_fold = (y_pred_fold_prob > 0.5).astype(int)
#     accuracy_fold = accuracy_score(y_test_fold, y_pred_fold)
#     accuracy_scores.append(accuracy_fold)

# average_accuracy = sum(accuracy_scores) / len(accuracy_scores)
# print(f"Average accuracy across folds: {average_accuracy:.4f}")



